# コンテキストウィンドウ設計改善

**ステータス**: 調査完了 → Phase 1 実施済み

---

## 1. 問題の概要

プレイテスト v2 で複数のペルソナから共通して報告された問題:

- **同じ問題の繰り返し出題**（x²-5x+6 が4回出題された例あり）
- **会話コンテキストの喪失**（Turn 9 で「元の問題を教えて」と聞き返す）
- **ハルシネーション**（「前のドラマ」等、会話に存在しない内容への言及）
- **終了意思の無視**（「もう寝る」と言っても新問題を出し続ける）

## 2. 現状分析

### 2.1 Gemini API に渡る情報の全体構成

```
┌─────────────────────────────────────────────────┐
│ systemInstruction (毎回固定で全量送信)            │
│                                                   │
│  BASE_SYSTEM_PROMPT        ~5,000字 (~6,500 tok)  │
│  ALL_FEW_SHOT_EXAMPLES     ~3,500字 (~4,500 tok)  │
│  介入戦略 (Lv1-5)          ~  400字 (~  500 tok)  │
│  オンボーディング          ~  200字 (~  250 tok)  │
│  ミスパターン              ~  270字 (~  350 tok)  │
│  スキル推薦                ~  600字 (~  800 tok)  │
│  [習得判定モード]          ~1,900字 (~2,400 tok)  │
│                                                   │
│  合計: 通常時 ~12,000 tok / 判定時 ~15,000 tok    │
└─────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────┐
│ contents (会話履歴 — 全メッセージ無制限送信)      │
│                                                   │
│  Turn 1: user (~50 tok) + model (~500-800 tok)    │
│  Turn 2: user (~50 tok) + model (~500-800 tok)    │
│  ...                                              │
│  Turn N: user (~50 tok) + model (~500-800 tok)    │
│                                                   │
│  1ターン ~600-900 tok（GuideAI は数式・段階的     │
│  解説を含むため応答が長い）                        │
│  10ターンで ~6,000-9,000 tok                      │
│  20ターンで ~12,000-18,000 tok                    │
│  画像含むと 1ターン ~2,000 tok 追加               │
└─────────────────────────────────────────────────┘
```

### 2.2 問題の根本原因

| # | 原因 | 影響 | 深刻度 |
|---|------|------|--------|
| A | **システムプロンプトが巨大**（~12K tok）| Gemini Flash の注意が分散し、会話履歴への注意力が低下 | 高 |
| B | **会話履歴を無制限に全量送信** | ターン数増加でプロンプト全体が膨張 → 注意がさらに分散 | 高 |
| C | **出題済み問題のトラッキングなし** | AI が過去に何を出題したか把握できず、同じ問題を繰り返す | 高 |
| D | **Few-shot 例が常時含まれる** | 通常チュータリング時にも ~4,500 tok の会話例が注入される | 中 |
| E | **会話の「要約」機構なし** | 古いターンの詳細情報が無駄にトークンを消費 | 中 |

### 2.3 トークン膨張の実態

**10ターン会話時の合計**:
- システムプロンプト: ~12,000 tok
- 会話履歴（10ターン）: ~6,000-9,000 tok
- **合計: ~18,000-21,000 tok**

**20ターン会話時の合計**:
- システムプロンプト: ~12,000 tok
- 会話履歴（20ターン）: ~12,000-18,000 tok
- **合計: ~24,000-30,000 tok**

プレイテストでは Turn 9 で既にコンテキスト喪失が観測されている（合計 ~17,000-20,000 tok 付近）。

### 2.4 Gemini Flash の注意力特性

Gemini Flash は高速だが、長大なコンテキストでは**中間部分への注意力が低下**する（Lost in the Middle 問題）。
特にシステムプロンプトが長いと、会話履歴の中間ターンが事実上「見えなくなる」。

---

## 3. 設計方針: CC方式のオンデマンドコンテキスト管理

### 3.1 設計原理

Claude Code (CC) がうまく会話を維持できる理由:

1. **システムプロンプトは「誰か」と「基本ルール」だけ** — 具体的な知識・プロトコルは入れない
2. **必要な情報はオンデマンドで注入** — 状況に応じて必要なプロトコルだけ追加
3. **古い会話は自動圧縮** — コンテキスト上限に近づくと要約に置き換える

現在の GuideAI は**真逆**: 全プロトコル・全 Few-shot・全スキル情報を毎ターン丸ごと送信。
「常に全部載せ」から「必要なものだけ、必要な時に」への転換が必要。

### 3.2 目標アーキテクチャ

```
┌──────────────────────────────────────────────────┐
│ systemInstruction                                 │
│                                                    │
│  コアプロンプト (常時)           ~3,000 tok         │
│  ┣ 人格・口調                                     │
│  ┣ 数式表示ルール                                 │
│  ┣ 基本禁止事項                                   │
│  ┗ タグ出力ルール (SKILL_MASTERY のみ)             │
│                                                    │
│  動的注入 (状況に応じて 0〜1 つだけ追加)            │
│  ┣ 習得判定モード時 → 判定プロトコル + Few-shot     │
│  ┣ 画像アップ時    → 参考書対応プロトコル           │
│  ┗ 通常会話時      → 追加なし                      │
│                                                    │
│  学習者コンテキスト (常時、ただし簡潔)   ~500 tok   │
│  ┣ 自立度レベル (1行)                              │
│  ┣ 現在スキル (1行)                                │
│  ┗ 注意ミスパターン (箇条書き数行)                  │
│                                                    │
│  合計: 通常時 ~3,500 tok / 判定時 ~6,000 tok       │
└──────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────┐
│ contents (会話履歴 — ウィンドウ管理)               │
│                                                    │
│  [会話サマリー] (6ターン超過時、user ロールで挿入)  │
│  "ここまでの要約: 因数分解を学習中。x²-5x+6,      │
│   x²+3x+2 を出題済み。2問とも正解。"    ~200 tok  │
│                                                    │
│  直近 6 ターン → フルテキスト        ~4,000-5,000  │
│                                                    │
│  合計: ~5,000 tok（ターン数に関わらず安定）         │
└──────────────────────────────────────────────────┘

Gemini への総入力: 通常時 ~8,500 tok（現状 ~20K の半分以下）
```

### 3.3 現状 → 目標の変更一覧

| 項目 | 現状 | 目標 | 削減 |
|------|------|------|------|
| BASE_SYSTEM_PROMPT | ~6,500 tok（全プロトコル込み）| ~3,000 tok（コア人格のみ）| -3,500 |
| Few-shot 例 | 常時注入 ~4,500 tok | 判定時のみ | -4,500 (通常時) |
| 介入戦略 | ~500 tok（全文）| ~50 tok（1行サマリー）| -450 |
| 旧方式プロトコル | ~1,000 tok（MASTERY_SCORE 等）| 削除 | -1,000 |
| 会話履歴 (10ターン) | ~8,000 tok（全量）| ~5,000 tok（直近6 + 要約）| -3,000 |
| **合計 (10ターン時)** | **~20,000 tok** | **~8,500 tok** | **-11,500** |

---

## 4. 実装ステップ

### Step 1: システムプロンプトの分離と圧縮

**目標**: `BASE_SYSTEM_PROMPT` をコア部分と状況別プロトコルに分離

#### 1a. コアプロンプト作成 (~3,000 tok)
- 役割定義・キャラクター・基本理念 → 残す
- 数式表示・色分けルール → 残す（簡潔化）
- 対話パターン 1-3 → 残す
- 禁止事項 → 簡潔化（5行 → 3行）
- `[[SKILL_MASTERY]]` タグルール → 残す（必須）
- 数学以外の話題 `[[OFF_TOPIC]]` → 残す（簡潔化）
- ミスへの対応 → 残す

#### 1b. 状況別プロトコルの分離
以下を `BASE_SYSTEM_PROMPT` から除外し、動的注入に:
- 旧方式 `[[MASTERY_SCORE:XX]]` プロトコル → **削除**（新方式に一本化）
- 参考書ページ対応プロトコル → 画像アップロード検出時のみ注入
- 参考書正誤判定プロトコル → 画像アップロード検出時のみ注入
- Few-shot 例 → 判定時のみ注入

#### 1c. 介入戦略の圧縮
- 現状: レベル別に ~500 tok の詳細文
- 改善: `## 介入レベル: Lv2（質問で導く。答えを教えず、ヒントで考えさせる）` の1行

#### 1d. 学習者コンテキストの圧縮
- 現状: 各セクションに見出し・説明文付き
- 改善: 構造化された簡潔なブロック
```
## 学習者情報
- 自立度: Lv2
- 現在スキル: 二次方程式 (I-QF-01)
- 注意: 書き写しミス多発（12回、増加傾向）
```

**工数**: 中（プロンプト書き換え + prompt-builder 条件分岐追加）
**リスク**: 低（動作を変えずにプロンプト構造を変えるだけ）

### Step 2: 動的プロトコル注入

**目標**: サーバー側で状況を判定し、必要なプロトコルだけ注入

#### 判定ロジック (server.ts)
```typescript
// 画像アップロードの検出
const hasImage = messages.some(m =>
  Array.isArray(m.content) && m.content.some(b => b.type === 'image')
);

// プロンプトオプションに状況を伝達
const promptOptions: PromptBuildOptions = {
  assessmentMode,                    // 既存: 習得判定モード
  includeImageProtocol: hasImage,    // NEW: 画像対応プロトコル
};
```

#### prompt-builder.ts の変更
```typescript
// コアプロンプト（常時）
prompt += CORE_SYSTEM_PROMPT;

// 動的注入（状況に応じて 0〜1 つ）
if (options?.assessmentMode) {
  prompt += ASSESSMENT_PROTOCOL;     // 判定プロトコル + Few-shot
} else if (options?.includeImageProtocol) {
  prompt += IMAGE_PROTOCOL;          // 参考書対応プロトコル
}
// 通常会話: 追加なし
```

**工数**: 小（条件分岐追加のみ）
**リスク**: 低

### Step 3: 会話履歴のウィンドウ管理

**目標**: 古いターンを要約し、直近ターンのみフルテキストで送信

#### 3a. サーバー側ウィンドウ処理 (server.ts)
```typescript
const FULL_HISTORY_WINDOW = 6;  // 直近6ターン（12メッセージ）をフル保持

if (messages.length > FULL_HISTORY_WINDOW * 2) {
  const oldMessages = messages.slice(0, -(FULL_HISTORY_WINDOW * 2));
  const recentMessages = messages.slice(-(FULL_HISTORY_WINDOW * 2));

  // 古いメッセージを要約（Gemini Flash で軽量生成）
  const summary = await summarizeConversation(oldMessages);

  // 要約を user ロールの最初のメッセージとして挿入
  const summaryMessage = {
    role: 'user',
    content: `[これまでの会話の要約]\n${summary}`
  };

  messages = [summaryMessage, ...recentMessages];
}
```

#### 3b. 要約プロンプト
```
以下の会話を3-5行で要約してください。
必ず含めること:
- 出題した問題（数式そのまま）
- 学習者の理解度
- 現在の学習状況
```

#### 3c. 出題済み問題トラッキング
要約とは別に、出題した問題のリストをシステムプロンプトに注入:
```
## 出題済み問題（重複禁止）
1. x²-5x+6=0 を因数分解せよ
2. 2x²+3x-5=0 を解け
```
- サーバー側で AI レスポンスから問題文パターンを抽出
- セッション中はメモリ保持、Firestore にも永続化

**工数**: 大（要約生成 + 問題抽出ロジック）
**リスク**: 中（要約品質に依存）

### Step 4: 会話ターン上限 + UI 通知

- 1会話あたりの推奨上限: 20ターン
- 15ターン時点で UI に「もうすぐ新しい会話を始めるのがおすすめです」と通知
- 学習進捗は Firestore に保存されているので、新会話でも継続可能

**工数**: 小
**リスク**: 低

---

## 5. 実装優先順位

| 順位 | ステップ | 工数 | 効果 | リスク |
|:---:|------|:---:|:---:|:---:|
| 1 | **Step 1: プロンプト分離・圧縮** | 中 | 大 | 低 |
| 2 | **Step 2: 動的プロトコル注入** | 小 | 大 | 低 |
| 3 | **Step 4: ターン上限 + UI 通知** | 小 | 中 | 低 |
| 4 | **Step 3: ウィンドウ管理 + 要約** | 大 | 大 | 中 |

### Phase 1（今回実施: Step 1 + 2）
- コアプロンプト作成（分離・圧縮）
- 動的注入の条件分岐
- **効果: システムプロンプト ~12K → ~3.5K tok（通常時）**
- **プレイテストで効果検証**

### Phase 1 結果（実施済み・マージ済み）
- コアプロンプト作成（分離・圧縮）
- 動的注入の条件分岐
- **効果: システムプロンプト ~12K → ~3.5K tok（通常時）**
- A/B テスト: sae (10ターン), aoi_sketch (14ターン) ともにコンテキスト喪失ゼロ
- テスト結果: スリムプロンプトテスト summary 参照

---

## Phase 2: 会話の要約とターン管理

### Phase 2-1: ユーザー主導の会話要約

**設計思想**: CC の auto-compact をヤングユーザー向けにアレンジ。
黙って圧縮するのではなく、**ユーザーに選ばせる**（誠実な UX）。

#### トリガー
- **トークン数ベース**（ターン数ではない）
- `generateContent` レスポンスの `usageMetadata.promptTokenCount` を利用（既に取得済み）
- 閾値: **input_tokens > 15,000 tok** で通知表示
  - 通常時 system ~3.5K + 会話 ~12K = ~15.5K → 約15-18ターン相当

#### UX フロー
```
[15,000 tok 超過時]
  ↓
バックエンド: レスポンスに { contextAlert: true, tokenUsage: 15234 } を付与
  ↓
フロントエンド: チャット内にバナー表示（ToastNotification 風）
  ┌─────────────────────────────────────────────────────┐
  │ 💬 会話が長くなってきたね！                           │
  │ ここまでの内容をまとめて、すっきりさせる？             │
  │                                                       │
  │  [まとめて続ける]    [このまま続ける]                  │
  └─────────────────────────────────────────────────────┘

[まとめて続ける] を選んだ場合:
  ↓
フロントエンド: POST /api/chat/summarize { messages, conversationId }
  ↓
バックエンド:
  1. Flash で会話を要約（3-5行 + 出題済み問題リスト）
  2. 要約メッセージを system role で先頭に配置
  3. 直近 4 ターン（8メッセージ）はフルテキスト保持
  4. Firestore の会話に要約マーカーを保存
  ↓
フロントエンド:
  - メッセージ一覧を更新（要約 + 直近ターン）
  - 「まとめました！続きからどうぞ」とトースト表示

[このまま続ける] を選んだ場合:
  ↓
フロントエンド: バナーを閉じる。以降は表示しない（同一セッション内）
```

#### バックエンド実装

**server.ts の変更**:
```typescript
// レスポンスに tokenUsage を追加（既存の usage から取得）
const inputTokens = response.usage?.input_tokens || 0;
const CONTEXT_ALERT_THRESHOLD = 15000;

res.json({
  ...extendedResponse,
  tokenUsage: inputTokens,
  contextAlert: inputTokens > CONTEXT_ALERT_THRESHOLD,
});
```

**新規エンドポイント POST /api/chat/summarize**:
```typescript
// 1. Flash で会話要約を生成
const summaryPrompt = `以下の会話を要約してください。
必ず含めること:
- 出題した問題（数式をそのまま記載）
- 学習者の理解度（何ができて何がまだ難しいか）
- 現在の学習トピック
3-5行で簡潔に。`;

// 2. 要約 + 直近4ターンを返す
const KEEP_RECENT_TURNS = 4;
const recentMessages = messages.slice(-(KEEP_RECENT_TURNS * 2));
const summaryMessage = { role: 'user', content: `[ここまでの学習の要約]\n${summary}` };
```

#### フロントエンド実装

**ChatInterface.tsx の変更**:
- `contextAlert` state を追加
- レスポンスから `contextAlert` を検知してバナー表示
- 「まとめて続ける」ボタンで `/api/chat/summarize` を呼び出し
- メッセージ状態を要約版で置き換え

**api-service.ts の変更**:
- `ExtendedChatResponse` に `tokenUsage`, `contextAlert` を追加
- `summarizeConversation()` 関数を追加

#### 影響範囲
- `prototype/src/server.ts` — contextAlert 付与 + summarize エンドポイント
- `webapp/src/components/ChatInterface.tsx` — バナー UI + まとめ機能
- `webapp/src/services/api-service.ts` — summarize API 呼び出し

#### 工数: 中
#### リスク: 低（既存フローを壊さない。オプトイン機能）

---

### Phase 2-2: ターン数表示（任意）

Phase 2-1 でトークンベースの管理ができれば、ターン上限 UI は不要かもしれない。
必要に応じて「Turn 15/20」のような表示を追加。工数: 小。

---

## Phase 2 の優先度評価: 30ターン閾値テスト

### テスト概要
Phase 2-1 の実装前に、スリムプロンプト版でコンテキスト劣化がどの程度のトークン数で発生するか API 直接テストで検証。

- **テストスクリプト**: `scripts/test-context-threshold.ts`
- **手法**: `/api/chat` に因数分解の学習会話を30ターン蓄積しながら POST。チェックポイントで「最初の問題は？」と聞いてコンテキスト保持を確認
- **チェックポイント**: Turn 20（最初の問題を覚えているか）、Turn 29（最初の問題＋最難問を覚えているか）

### 結果

| 指標 | 値 |
|------|-----|
| 総ターン数 | 30 |
| 最大 input_tokens | **~14,300 tok** |
| Turn 20 チェック | **PASS** — 2x+2y を正確に記憶 |
| Turn 29 チェック | **PASS** — 最初の問題・最難問ともに言及 |
| 劣化ポイント | **なし（30ターンで劣化せず）** |

### トークン推移
- Turn 1: ~1,800 tok → Turn 10: ~5,800 tok → Turn 20: ~10,000 tok → Turn 30: ~14,300 tok
- 1ターンあたり約 400-500 tok の増加（スリムプロンプトで応答が簡潔になった効果）

### 結論
- Gemini Flash のコンテキストウィンドウは **1M tok**。30ターン/14,300 tok は **わずか 1.4%**
- スリムプロンプト（~3.5K tok）により、会話履歴への注意力が十分確保されている
- **Phase 2-1（ユーザー主導の会話要約）は緊急ではない**
  - 実装設計は上記に残すが、優先度を下げる
  - 50ターン超・50K tok 超のセッションが現実的になった段階で再検討

---

## 6. 検証方法

1. **Phase 1 検証（完了）**: A/B テストで改善確認済み（コンテキスト喪失ゼロ、問題重複ゼロ）
2. **閾値テスト（完了）**: 30ターン/14,300 tok で劣化なし。Phase 2-1 は急がない
3. **回帰テスト**: 習得判定モードが正しく動作するか（Few-shot 注入確認）

---

## 7. 参考

- Gemini API ドキュメント: コンテキストウィンドウ管理
- "Lost in the Middle" 論文: 長文コンテキストでの注意力低下
- LangChain ConversationSummaryBufferMemory: トークンベースの要約トリガー
- Claude Code auto-compact: コンテキスト 75% で発動、ユーザー `/compact` で手動実行可
- スリムプロンプトで関連 Issue 解消済み
- プレイテストレポート: スリムプロンプトテスト summary 参照
